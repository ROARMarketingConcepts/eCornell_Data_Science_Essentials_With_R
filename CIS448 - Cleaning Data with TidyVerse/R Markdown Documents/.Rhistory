}
## Returns the total probability mass discounted from all observed trigrams.
## This is the amount of probability mass which is
## redistributed to UNOBSERVED trigrams. If no trigrams starting with
## bigram$ngram[1] exist, 1 is returned.
##
## obsTrigs - 2 column data.frame or data.table. The first column: ngram,
##            contains all the observed trigrams that start with the bigram
##            prefix we are attempting to the predict the next word of. The
##            second column: freq, contains the frequency/count of each trigram.
## bigram - single row frequency table where the first col: ngram, is the bigram
##          which are the first two words of unobserved trigrams we want to
##          estimate probabilities of (same as bigPre in other functions listed
##          prior) delimited with '_'. The second column: freq, is the
##          frequency/count of the bigram listed in the ngram column.
## triDisc - amount to discount observed trigrams
getAlphaTrigram <- function(obsTrigs, bigram, triDisc=0.5) {
if(nrow(obsTrigs) < 1) return(1)
alphaTri <- 1 - sum((obsTrigs$freq - triDisc) / bigram$freq[1])
return(alphaTri)
}
## Returns a dataframe of 2 columns: ngram and prob.  Values in the ngram
## column are unobserved trigrams of the form: w3_w2_w1.  The values in the prob
## column are q_bo(w1 | w3, w2) calculated from equation 17.
##
## bigPre -  single-element char array of the form w2_w1 which are first two
##           words of the trigram we are predicting the tail word of
## qboObsBigrams - 2 column data.frame with the following columns -
##                 ngram: observed bigrams of the form w2_w1
##                 probs: the probability estimate for observed bigrams:
##                        qbo(w1 | w2) calc'd from equation 10.
## qboUnobsBigrams - 2 column data.frame with the following columns -
##                   ngram: unobserved bigrams of the form w2_w1
##                   probs: the probability estimate for unobserved bigrams
##                          qbo(w1 | w2) calc'd from equation 16.
## alphaTrig - total discounted probability mass at the trigram level
getUnobsTriProbs <- function(bigPre, qboObsBigrams,
qboUnobsBigrams, alphaTrig) {
qboBigrams <- rbind(qboObsBigrams, qboUnobsBigrams)
qboBigrams <- qboBigrams[order(-qboBigrams$prob), ]
sumQboBigs <- sum(qboBigrams$prob)
first_bigPre_word <- str_split(bigPre, "_")[[1]][1]
unobsTrigNgrams <- paste(first_bigPre_word, qboBigrams$ngram, sep="_")
unobsTrigProbs <- alphaTrig * qboBigrams$prob / sumQboBigs
unobsTrigDf <- data.frame(ngram=unobsTrigNgrams, prob=unobsTrigProbs)
return(unobsTrigDf)
}
getPredictionMsg <- function(qbo_trigs) {
# pull off tail word of highest prob trigram
prediction <- str_split(qbo_trigs$ngram[1], "_")[[1]][3]
result <- sprintf("%s%s%s%.4f", "highest prob prediction is >>> ", prediction," <<< which has probability = ", qbo_trigs$prob[1])
return(result)
}
predict_word <- function(bigPre,unigrs,bigrs,trigrs){
gamma2=0.7; gamma3=0.7  # initialize new discount rates
obs_trigs <- getObsTrigs(bigPre, trigrs)
unobs_trig_tails <- getUnobsTrigTails(obs_trigs$ngram, unigrs)
bo_bigrams <- getBOBigrams(bigPre, unobs_trig_tails)
# Separate bigrams into observed and unobserved using the appropriate equations
obs_bo_bigrams <- getObsBOBigrams(bigPre, unobs_trig_tails, bigrs)
unobs_bo_bigrams <- getUnobsBOBigrams(bigPre, unobs_trig_tails, obs_bo_bigrams)
# Calculate observed bigram probabilites
qbo_obs_bigrams <- getObsBigProbs(obs_bo_bigrams, unigrs, gamma2)
# Calculate alpha_big and unobserved bigram probabilities
unig <- str_split(bigPre, "_")[[1]][2]
unig <- unigrs[unigrs$ngram == unig,]
alpha_big <- getAlphaBigram(unig, bigrs, gamma2)
# Distribute discounted bigram probability mass to unobserved bigrams in   proportion to unigram ML
qbo_unobs_bigrams <- getQboUnobsBigrams(unobs_bo_bigrams, unigrs, alpha_big)
# Calculate observed trigram probabilities...
qbo_obs_trigrams <- getObsTriProbs(obs_trigs, bigrs, bigPre, gamma3)
# Finally, calculate unobserved trigram probabilities...
bigram <- bigrs[bigrs$ngram %in% bigPre, ]
alpha_trig <- getAlphaTrigram(obs_trigs, bigram, gamma3)
qbo_unobs_trigrams <- getUnobsTriProbs(bigPre, qbo_obs_bigrams,
qbo_unobs_bigrams, alpha_trig)
qbo_trigrams <- rbind(qbo_obs_trigrams, qbo_unobs_trigrams)
qbo_trigrams <- qbo_trigrams[order(-qbo_trigrams$prob), ]
# getPredictionMsg(qbo_trigrams)
return(qbo_trigrams)
}
bigPre = "bouquet_case"
word_list <- predict_word(bigPre,unigrs,bigrs,trigrs)
# word_list[grepl("bouquet_case_beer", ngram, fixed=TRUE)]
word_list[((word_list$ngram=="bouquet_case_beer") | (word_list$ngram=="bouquet_case_pretzels") | (word_list$ngram=="bouquet_case_cheese")| (word_list$ngram=="bouquet_case_soda")),]
bigPre = "would_mean"
word_list <- predict_word(bigPre,unigrs,bigrs,trigrs)
word_list[((word_list$ngram=="would_mean_world") | (word_list$ngram=="would_mean_best") | (word_list$ngram=="would_mean_most")| (word_list$ngram=="would_mean_universe")),]
bigPre = "make_me"
word_list <- predict_word(bigPre,unigrs,bigrs,trigrs)
word_list[((word_list$ngram=="make_me_bluest") | (word_list$ngram=="make_me_smelliest") | (word_list$ngram=="make_me_saddest")| (word_list$ngram=="make_me_happiest")),]
install.packages("rnn")
?rnn
??rnn
run.rnn_demo
library(rnn)
run.rnn_demo
install.packages("keras")
install.packages("tensorflow")
shiny::runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
# Load the raw data files.
# These lines of code will take a little time to execute, so please be patient!
NEI <- readRDS("exdata-data-NEI_data/summarySCC_PM25.rds")
SCC <- readRDS("exdata-data-NEI_data/Source_Classification_Code.rds")
merged_df <- merge(NEI,SCC,by="SCC")
total_emissions <- aggregate(NEI$Emission, by=list(NEI$year), sum)
plot(total_emissions,pch=16,xlab="Year",ylab="Emissions (tons)",main="Total Emissions by Year")
lines(total_emissions$Group.1,total_emissions$x)
grid(lty="dotted")
echo = TRUE
# First, we run the following line of code to clean up the memory from any previous R sessions.
rm(list=ls(all=TRUE))
# Load the raw dataset
df <- read.csv("repdata-data-StormData.csv")
echo = TRUE
# Let's look at a few rows of the df...
head(df)
echo = TRUE
library(dplyr)
# Add numbers in 'FATALITIES' and 'INJURIES' columns
df$CASUALTIES <- df$FATALITIES+df$INJURIES
# Let's sort the df by 'CASUALTIES' descending
df <- df %>% arrange(-CASUALTIES)
echo = TRUE
# Aggregate the casualties by event type
casualties_per_event <- aggregate(df$CASUALTIES, by=list(df$EVTYPE), sum)
names(casualties_per_event) <- c("EVTYPE", "TOTAL_CASUALTIES")
casualties_per_event <- casualties_per_event %>% arrange(-TOTAL_CASUALTIES)
# Filter for events where the total casualties is greater than 1000
casualties_per_event <- casualties_per_event[casualties_per_event$TOTAL_CASUALTIES >= 1000,]
# Plot a column chart of Total Casualties vs. Event Type
library(ggplot2)
library(scales)
# png("total_casualties_by_event_type.png")
ggplot(data=casualties_per_event, aes(x = reorder(EVTYPE, -TOTAL_CASUALTIES), y = TOTAL_CASUALTIES)) + geom_bar(position="dodge",stat="identity",color="blue") + xlab("Event Type") + ylab("Number of Casualities") + theme(axis.text.x = element_text(angle = 270,size=15,vjust=0.5,color="blue"), axis.text.y = element_text(size=15,color="blue"),plot.title = element_text(size = 25, face = "bold",hjust=0.5),axis.title = element_text(size=15,face="bold")) + ggtitle("Total Casualties by Event Type") + geom_text(aes(label=TOTAL_CASUALTIES), position=position_dodge(width=0.5), size=5, vjust=-2)+scale_y_continuous(label=comma)
# dev.off()
echo = TRUE
# First, we run the following line of code to clean up the memory from any previous R sessions.
rm(list=ls(all=TRUE))
# Load the raw dataset
df <- read.csv("repdata-data-StormData.csv")
echo = TRUE
# Let's look at a few rows of the df...
head(df)
echo = TRUE
library(dplyr)
# Add numbers in 'FATALITIES' and 'INJURIES' columns
df$CASUALTIES <- df$FATALITIES+df$INJURIES
# Let's sort the df by 'CASUALTIES' descending
df <- df %>% arrange(-CASUALTIES)
echo = TRUE
# Aggregate the casualties by event type
casualties_per_event <- aggregate(df$CASUALTIES, by=list(df$EVTYPE), sum)
names(casualties_per_event) <- c("EVTYPE", "TOTAL_CASUALTIES")
casualties_per_event <- casualties_per_event %>% arrange(-TOTAL_CASUALTIES)
# Filter for events where the total casualties is greater than 1000
casualties_per_event <- casualties_per_event[casualties_per_event$TOTAL_CASUALTIES >= 1000,]
# Plot a column chart of Total Casualties vs. Event Type
library(ggplot2)
library(scales)
# png("total_casualties_by_event_type.png")
ggplot(data=casualties_per_event, aes(x = reorder(EVTYPE, -TOTAL_CASUALTIES), y = TOTAL_CASUALTIES)) + geom_bar(position="dodge",stat="identity",color="blue") + xlab("Event Type") + ylab("Number of Casualities") + theme(axis.text.x = element_text(angle = 270,size=15,vjust=0.5,color="blue"), axis.text.y = element_text(size=15,color="blue"),plot.title = element_text(size = 25, face = "bold",hjust=0.5),axis.title = element_text(size=15,face="bold")) + ggtitle("Total Casualties by Event Type") + geom_text(aes(label=TOTAL_CASUALTIES), position=position_dodge(width=0.5), size=5, vjust=-2)+scale_y_continuous(label=comma)
# dev.off()
plot(cars)
library(lubridate)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(chron)
# Read in the data file.
df <- read.csv("activity.csv")
# Convert 'date' column to datetime variable.
df$date <- as.Date(df$date,format="%Y-%m-%d")
# Convert 'interval' to a time variable
# df$interval <- times(sub("(.{2})", "\\1:", sprintf("%04d:00", df$interval)))
knitr::opts_chunk$set(echo = TRUE)
library(chron)
# Read in the data file.
df <- read.csv("activity.csv")
# Convert 'date' column to datetime variable.
df$date <- as.Date(df$date,format="%Y-%m-%d")
# Convert 'interval' to a time variable
# df$interval <- times(sub("(.{2})", "\\1:", sprintf("%04d:00", df$interval)))
# Drop rows with NA in 'steps' column
df1 <- df[complete.cases(df),]
# Calculate total steps per day
total_steps_per_day <- aggregate(df1$steps, by=list(df1$date), sum)
# Generate histogram
# png("Histogram of Total Steps Per Day.png")
hist(total_steps_per_day$x,breaks=20,col="blue",xlab="Total Steps Per Day",main="Histogram of Total Steps Per Day")
# dev.off()
# Calculate and report the mean and median of the total number of steps taken per day
mean <- mean(total_steps_per_day$x)
median <- median(total_steps_per_day$x)
paste0("Mean Total Steps per Day: ",round(mean,2))
paste0("Median Total Steps per Day: ",round(median,2))
library(ggplot2)
# Calculate mean number of steps for each time period
mean_steps <- aggregate(df1$steps, by=list(df1$interval), mean)
names(mean_steps) <- c("time","steps")
# Plot the line graph
# png("Mean Number of Steps by Time of Day.png")
ggplot(data=mean_steps, aes(x = time, y = steps))+geom_line()+xlab("Time of Day (HHMM)")+ggtitle("Mean Number of Steps by Time of Day")
# dev.off()
# Determine time interval when mean number of steps is maximum
mean_steps[which.max(mean_steps$steps), ]$time
# Calculate the number of rows with NAs
sum(!complete.cases(df))
library(plyr)
library(dplyr)
# Get rows in df where NAs are present
na_rows <- df[!complete.cases(df),]
# Rename columns
names(na_rows) <- c("steps","date","time")
# Impute mean number of steps for each time interval
na_rows <- merge(na_rows,mean_steps, by=c("time"))
# 'df1' is the original dataset with the NA rows deleted. We need to combine 'na_rows' with 'df1'. However, we need to clean up 'na_rows' a bit...
na_rows = subset(na_rows, select = -c(steps.x) )
names(na_rows) <- c("interval","date","steps")
# Combine 'df1' and 'na_rows'
df1 <- union(df1,na_rows)
# Sort 'df1' by date and then by interval
df1 <- df1 %>% arrange(date, interval)
# Calculate total steps per day
total_steps_per_day <- aggregate(df1$steps, by=list(df1$date), sum)
# Generate histogram
# png("Histogram of Total Steps Per Day with Imputed Values.png")
hist(total_steps_per_day$x,breaks=20,col="blue",xlab="Total Steps Per Day",main="Histogram of Total Steps Per Day")
# dev.off()
# Calculate and report the mean and median of the total number of steps taken per day
mean <- mean(total_steps_per_day$x)
median <- median(total_steps_per_day$x)
paste0("Mean Total Steps per Day: ",round(mean,2))
paste0("Median Total Steps per Day: ",round(median,2))
# Determine day of week for each date
df1$day <- weekdays(df1$date)
# Recode day of week to 'Weekday' or 'Weekend'
df1$weekday_or_weekend[df1$day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday")] <- "Weekday"
df1$weekday_or_weekend[df1$day %in% c("Saturday","Sunday")] <- "Weekend"
# Calculate mean number of steps for each time interval on Weekdays and Weekends
mean_steps <- aggregate(df1$steps, by=list(df1$weekday_or_weekend,df1$interval), mean)
names(mean_steps) <- c("Weekday_Weekend","time","steps")
# Sort 'mean_steps' by weekday/weekend and then by time
mean_steps <- mean_steps %>% arrange(Weekday_Weekend, time)
# The 'Weekday_Weekend' column must be converted to a factor variable for plotting
mean_steps$Weekday_Weekend <- as.factor(mean_steps$Weekday_Weekend)
# Create plots
# png("Mean Number of Steps by Time of Day Weekday Weekend.png")
ggplot(mean_steps, aes(x=time, y=steps))+geom_line()+facet_wrap(~Weekday_Weekend,nrow=2)+xlab("Time of Day")+ylab("Mean steps")+ggtitle("Mean Number of Steps by Time of Day")
# dev.off()
View(bigrs)
View(word_list)
demo()
knitr::opts_chunk$set(echo = TRUE)
# eCornell Hex Codes:
crimson = '#b31b1b' #Crimson
lightGray = '#cecece' #lightGray
darkGray = '#606366'
skyBlue = '#92b2c4' #skyblue
gold = '#fbb040' #gold
ecBlack = '#393f47' #ecBlack
# Load the data.
school = read.csv('mrc_table2.csv', header = TRUE, check.names = FALSE)
school = school[,names(school) %in%
c('name', 'type', 'tier', 'tier_name', 'mr_kq5_pq1',
'par_median', 'k_median')]
names(school)[5:7] <- c("mobility_rate", "parent_income", "student_income")
## Calculate the mean parent income for students at highly selective private schools:
school = school[school$tier_name %in% c("Highly selective private", "Highly selective public"),]
par.income.prv = school$parent_income[school$tier_name == 'Highly selective private']
mean(par.income.prv)
## Calculate the mean parent income for students at highly selective public schools.
par.income.pub = school$parent_income[school$tier_name == 'Highly selective public']
mean(par.income.pub)
# Create the boxplot:
boxplot(par.income.prv, par.income.pub, names = c('Highly selective private', 'Highly selective public'), ylab = 'Parent Income (USD)',
main = 'Parent Income: HS private and public schools', col = ecBlack)
# Create a permutation distribution:
set.seed(1)
P = 10000
store_mean_diff = rep(0, P)
for (n in 1:P){
school.perm = school
school.perm$parent_income = sample(school.perm$parent_income, replace = FALSE)
school.perm.prv = school.perm$parent_income[school.perm$tier_name == 'Highly selective private']
school.perm.pub = school.perm$parent_income[school.perm$tier_name == 'Highly selective public']
store_mean_diff[n] = mean(school.perm.prv) - mean(school.perm.pub)
}
# Plot the observed sample statistic on the histogram:
hist(store_mean_diff, breaks = 20, freq = FALSE, col = ecBlack, border = 'white',
xlab = 'Mean Diff of Parent Income',
main = 'Histogram of Parent Income Diff (Permuted Data)')
install.packages('tidyverse')
library(tidyverse)
library(tidyverse)
install.packages("tidyverse")
library("tidyverse")
install.packages("tidyverse")
help()
install.packages(c("nycflights13", "gapminder", "Lahman"))
myplot <- mpg
mpg
mpg
# install.packages("tidyverse")
library("tidyverse")
help()
install.packages(c("nycflights13", "gapminder", "Lahman"))
myplot <- load(mpg)
myplot <- load('mpg')
str(mpg)
myplot = mpg
View(myplot)
View(myplot)
myplot <- mpg
library(tidyverse)
library(lubridate)
library(utf8)
calls<-read.csv("calls_CQ1_Q2.csv", stringsAsFactors = F)
setwd("~/Desktop/Machine Learning and Data Analysis/eCornell_Data_Science_Essentials_With_R/CIS448 - Cleaning Data with TidyVerse/R Markdown Documents")
calls<-read.csv("calls_CQ1_Q2.csv", stringsAsFactors = F)
library(tidyverse)
library(lubridate)
library(utf8)
calls<-read.csv("calls_CQ1_Q2.csv", stringsAsFactors = F)
calls$address <- str_replace(calls$address, " & ", " @ ")
calls<-read.csv("calls_CQ1_Q2.csv", stringsAsFactors = F)
calls$reason <- str_trim(calls$reason)
calls$towns <- str_trim(calls$towns)
calls$time <- str_trim(calls$time)
calls$date <- str_trim(calls$date)
calls<-read.csv("calls_CQ1_Q2.csv", stringsAsFactors = F)
calls$reason <- tolower(calls$reason)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
calls <- read.csv("calls.csv", stringsAsFactors = FALSE)
# Initialize vectors to store each of the new variables
address <- c()
town <- c()
dt <- c()
for(i in 1:nrow(calls)) { # loop over emergency calls
# get the description of the i^th call
callI <- calls[i, "desc"]
# split the description text based on ";" --> gives a matrix of substrings
splitCallDesc <- str_split(callI, ";", simplify = TRUE)
# store the street address, town, and date/time
address[i] <- splitCallDesc[1]
town[i] <- splitCallDesc[2]
dt[i] <- splitCallDesc[3]
}
# add the new variables to the data frame
calls$address <- address
calls$towns <- town
calls$dt <- dt
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
calls <- read.csv("calls.csv", stringsAsFactors = FALSE)
# Initialize vectors to store each of the new variables
address <- c()
town <- c()
dt <- c()
for(i in 1:nrow(calls)) { # loop over emergency calls
# get the description of the i^th call
callI <- calls[i, "desc"]
# split the description text based on ";" --> gives a matrix of substrings
splitCallDesc <- str_split(callI, ";", simplify = TRUE)
# store the street address, town, and date/time
address[i] <- splitCallDesc[1]
town[i] <- splitCallDesc[2]
dt[i] <- splitCallDesc[3]
}
# add the new variables to the data frame
calls$address <- address
calls$towns <- town
calls$dt <- dt
View(calls)
temp <- str_which(calls$address, " @ ") # how many locations have an intersection
#(remember that "@" is used to denote an intersection)
length(temp) # how many calls have an intersection
library(tidyverse)
library(lubridate)
library(utf8)
calls <- read.csv("calls_CQ1_Q2.csv", stringsAsFactors = FALSE)
calls$address<- str_replace(calls$address, " & ", " @ ")
indices <- str_which(calls$reason, "CARDIAC")
reasons = calls[indices,]$reason
temp <- str_which(calls$address, " @ ") # how many locations have an intersection
#(remember that "@" is used to denote an intersection)
length(temp) # how many calls have an intersection
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
calls <- read.csv("911_calls.csv", stringsAsFactors = FALSE)
library(tidyverse)
calls <- read.csv("calls.csv", stringsAsFactors = FALSE)
# Prepare data:
# Initialize vectors to store each of the new variables
address <- c()
town <- c()
dt <- c()
for(i in 1:nrow(calls)) { # loop over emergency calls
# get the description of the i^th call
callI <- calls[i, "desc"]
# split the description text based on ";" --> gives a matrix of substrings
splitCallDesc <- str_split(callI, ";", simplify = TRUE)
# store the street address, town, and date/time
address[i] <- splitCallDesc[1]
town[i] <- splitCallDesc[2]
dt[i] <- splitCallDesc[3]
}
# add the new variables to the data frame
calls$address <- address
calls$towns <- town
calls$dt <- dt
# For plot:
theme <-  theme(plot.margin = margin(5, 5, 5, 5, "pt"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
panel.border = element_rect(colour = "#393f47", fill = NA, size = 2),
axis.text = element_text(size = 30),
axis.title.x = element_text(size = 30),
axis.title.y = element_text(size = 30),
plot.title = element_text(face = "bold", size = 30))
ourTheme <- list(theme, scale_size_manual(values = c(1.5)))
head(calls$dt)
library(lubridate)
# convert date-times from strings to POSIXct objects
# need to specify the order in which the components appear -- in this case, year-month-day-hour-minute-second
calls$dtBetter <- ymd_hms(calls$dt)
head(calls$dtBetter)
# convert date-times from strings to POSIX objects, but with the time zone specified
temp <- ymd_hms(calls$dt, tz = "America/New_York")
head(temp)
# Date and time of first call
firstCall <- calls$dtBetter[1]
firstCall
# Pull out components
year(firstCall)
month(firstCall)
day(firstCall)
hour(firstCall)
minute(firstCall)
second(firstCall)
# create a new variable called "hour" that gives the hour (0-23) of the emergency call
calls$hour <- hour(calls$dtBetter)
head(calls)
# make a barplot of the number of calls in each hour
ggplot(calls, aes(x = hour)) + geom_bar(fill = "#b31b1b") + ourTheme
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
calls <- read.csv("calls.csv", stringsAsFactors = FALSE)
# Initialize vectors to store each of the new variables
address <- c()
town <- c()
dt <- c()
for(i in 1:nrow(calls)) { # loop over emergency calls
# get the description of the i^th call
callI <- calls[i, "desc"]
# split the description text based on ";" --> gives a matrix of substrings
splitCallDesc <- str_split(callI, ";", simplify = TRUE)
# store the street address, town, and date/time
address[i] <- splitCallDesc[1]
town[i] <- splitCallDesc[2]
dt[i] <- splitCallDesc[3]
}
# add the new variables to the data frame
calls$address <- address
calls$towns <- town
calls$dt <- dt
calls$dt
calls$dtBetter <- ymd_hms(calls$dt)
# specify the start and end of the interval as POSIXct objects
start <- ymd_hms("2020-01-01 00:00:00")
end <- ymd_hms("2020-01-03 23:59:00")
# create the interval
newYears <- interval(start, end)
newYears
calls %>% filter(dtBetter %within% newYears)
# create interval from 1st to 2nd emergency call
firstToSecond <- interval(calls$dtBetter[1], calls$dtBetter[2])
as.duration(firstToSecond)
head(calls$dt)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
calls <- read.csv("calls.csv", stringsAsFactors = FALSE)
# Initialize vectors to store each of the new variables
address <- c()
town <- c()
dt <- c()
for(i in 1:nrow(calls)) { # loop over emergency calls
# get the description of the i^th call
callI <- calls[i, "desc"]
# split the description text based on ";" --> gives a matrix of substrings
splitCallDesc <- str_split(callI, ";", simplify = TRUE)
# store the street address, town, and date/time
address[i] <- splitCallDesc[1]
town[i] <- splitCallDesc[2]
dt[i] <- splitCallDesc[3]
}
# add the new variables to the data frame
calls$address <- address
calls$towns <- town
calls$dt <- dt
head(calls$dt)
calls$dtBetter <- ymd_hms(calls$dt)
calls %>% filter(dtBetter %within% newYears)
head(calls)
