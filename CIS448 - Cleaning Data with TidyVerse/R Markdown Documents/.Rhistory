trigram_indices <- grep(regex, trigrams$ngram)
if(length(trigram_indices) > 0) {
trigs.winA <- trigrams[trigram_indices, ]
}
return(trigs.winA)
}
## Returns a two column data.frame of observed trigrams that start with bigram
## prefix bigPre in the first column named ngram and the probabilities
## q_bo(w_i | w_i-2, w_i-1) in the second column named prob calculated from
## eqn 12. If no observed trigrams starting with bigPre exist, NULL is returned.
##
## obsTrigs - 2 column data.frame or data.table. The first column: ngram,
##            contains all the observed trigrams that start with the bigram
##            prefix bigPre which we are attempting to the predict the next
##            word of in a give phrase. The second column: freq, contains the
##            frequency/count of each trigram.
## bigrs - 2 column data.frame or data.table. The first column: ngram,
##         contains all the bigrams in the corpus. The second column:
##         freq, contains the frequency/count of each bigram.
## bigPre -  single-element char array of the form w2_w1 which are first two
##           words of the trigram we are predicting the tail word of
## triDisc - amount to discount observed trigrams
getObsTriProbs <- function(obsTrigs, bigrs, bigPre, triDisc=0.5) {
if(nrow(obsTrigs) < 1) return(NULL)
obsCount <- subset(bigrs, ngram == bigPre)$freq[1]
obsTrigProbs <- mutate(obsTrigs, freq=((freq - triDisc) / obsCount))
colnames(obsTrigProbs) <- c("ngram", "prob")
return(obsTrigProbs)
}
## Returns a character vector which are the tail words of unobserved trigrams
## that start with the first two words of obsTrigs (aka the bigram prefix).
## These are the words w in the set B(w_i-2, w_i-1) as defined in the section
## describing the details of equation 17.
##
## obsTrigs - character vector of observed trigrams delimited by _ of the form:
##            w3_w2_w1 where w3_w2 is the bigram prefix
## unigs - 2 column data.frame of all the unigrams in the corpus:
##         ngram = unigram
##         freq = frequency/count of each unigram
getUnobsTrigTails <- function(obsTrigs, unigs) {
obs_trig_tails <- str_split_fixed(obsTrigs, "_", 3)[, 3]
unobs_trig_tails <- unigs[!(unigs$ngram %in% obs_trig_tails), ]$ngram
return(unobs_trig_tails)
}
## Returns the total probability mass discounted from all observed bigrams
## calculated from equation 14.  This is the amount of probability mass which
## is redistributed to UNOBSERVED bigrams. If no bigrams starting with
## unigram$ngram[1] exist, 0 is returned.
##
## unigram - single row, 2 column frequency table. The first column: ngram,
##           contains the w_i-1 unigram (2nd word of the bigram prefix). The
##           second column: freq, contains the frequency/count of this unigram.
## bigrams - 2 column data.frame or data.table. The first column: ngram,
##           contains all the bigrams in the corpus. The second column:
##           freq, contains the frequency or count of each bigram.
## bigDisc - amount to discount observed bigrams
getAlphaBigram <- function(unigram, bigrams, bigDisc=0.5) {
# get all bigrams that start with unigram
regex <- sprintf("%s%s%s", "^", unigram$ngram[1], "_")
bigsThatStartWithUnig <- bigrams[grep(regex, bigrams$ngram),]
if(nrow(bigsThatStartWithUnig) < 1) return(0)
alphaBi <- 1 - (sum(bigsThatStartWithUnig$freq - bigDisc) / unigram$freq)
return(alphaBi)
}
## Returns a character vector of backed off bigrams of the form w2_w1. These
## are all the (w_i-1, w) bigrams where w_i-1 is the tail word of the bigram
## prefix bigPre and w are the tail words of unobserved bigrams that start with
## w_i-1.
##
## bigPre - single-element char array of the form w2_w1 which are first two
##          words of the trigram we are predicting the tail word of
## unobsTrigTails - character vector that are tail words of unobserved trigrams
getBOBigrams <- function(bigPre, unobsTrigTails) {
w_i_minus1 <- str_split(bigPre, "_")[[1]][2]
boBigrams <- paste(w_i_minus1, unobsTrigTails, sep = "_")
return(boBigrams)
}
## Returns a two column data.frame of backed-off bigrams in the first column
## named ngram and their frequency/counts in the second column named freq.
##
## bigPre -  single-element char array of the form w2_w1 which are first two
##           words of the trigram we are predicting the tail word of
## unobsTrigTails - character vector that are tail words of unobserved trigrams
## bigrs - 2 column data.frame or data.table. The first column: ngram,
##         contains all the bigrams in the corpus. The second column:
##         freq, contains the frequency/count of each bigram.
getObsBOBigrams <- function(bigPre, unobsTrigTails, bigrs) {
boBigrams <- getBOBigrams(bigPre, unobsTrigTails)
obs_bo_bigrams <- bigrs[bigrs$ngram %in% boBigrams, ]
return(obs_bo_bigrams)
}
## Returns a character vector of backed-off bigrams which are unobserved.
##
## bigPre -  single-element char array of the form w2_w1 which are first two
##           words of the trigram we are predicting the tail word of
## unobsTrigTails - character vector that are tail words of unobserved trigrams
## obsBOBigram - data.frame which contains the observed bigrams in a column
##               named ngram
getUnobsBOBigrams <- function(bigPre, unobsTrigTails, obsBOBigram) {
boBigrams <- getBOBigrams(bigPre, unobsTrigTails)
unobs_bigs <- boBigrams[!(boBigrams %in% obsBOBigram$ngram)]
return(unobs_bigs)
}
## Returns a dataframe of 2 columns: ngram and probs.  Values in the ngram
## column are bigrams of the form: w2_w1 which are observed as the last
## two words in unobserved trigrams.  The values in the prob column are
## q_bo(w1 | w2) calculated from from equation 10.
##
## obsBOBigrams - a dataframe with 2 columns: ngram and freq. The ngram column
##                contains bigrams of the form w1_w2 which are observed bigrams
##                that are the last 2 words of unobserved trigrams (i.e. "backed
##                off" bigrams). The freq column contains integers that are
##                the counts of these observed bigrams in the corpus.
## unigs - 2 column data.frame of all the unigrams in the corpus:
##         ngram = unigram
##         freq = frequency/count of each unigram
## bigDisc - amount to discount observed bigrams
getObsBigProbs <- function(obsBOBigrams, unigs, bigDisc=0.5) {
first_words <- str_split_fixed(obsBOBigrams$ngram, "_", 2)[, 1]
first_word_freqs <- unigs[unigs$ngram %in% first_words, ]
obsBigProbs <- (obsBOBigrams$freq - bigDisc) / first_word_freqs$freq
obsBigProbs <- data.frame(ngram=obsBOBigrams$ngram, prob=obsBigProbs)
return(obsBigProbs)
}
## Returns a dataframe of 2 columns: ngram and prob.  Values in the ngram
## column are unobserved bigrams of the form: w2_w1.  The values in the prob
## column are the backed off probability estimates q_bo(w1 | w2) calculated
## from from equation 16.
##
## unobsBOBigrams - character vector of unobserved backed off bigrams
## unigs - 2 column data.frame of all the unigrams in the corpus:
##         ngram = unigram
##         freq = frequency/count of each unigram
## alphaBig - total discounted probability mass at the bigram level
getQboUnobsBigrams <- function(unobsBOBigrams, unigs, alphaBig) {
# get the unobserved bigram tails
qboUnobsBigs <- str_split_fixed(unobsBOBigrams, "_", 2)[, 2]
w_in_Aw_iminus1 <- unigs[!(unigs$ngram %in% qboUnobsBigs), ]
# convert to data.frame with counts
qboUnobsBigs <- unigs[unigs$ngram %in% qboUnobsBigs, ]
denom <- sum(qboUnobsBigs$freq)
# converts counts to probabilities
qboUnobsBigs <- data.frame(ngram=unobsBOBigrams,
prob=(alphaBig * qboUnobsBigs$freq / denom))
return(qboUnobsBigs)
}
## Returns the total probability mass discounted from all observed trigrams.
## This is the amount of probability mass which is
## redistributed to UNOBSERVED trigrams. If no trigrams starting with
## bigram$ngram[1] exist, 1 is returned.
##
## obsTrigs - 2 column data.frame or data.table. The first column: ngram,
##            contains all the observed trigrams that start with the bigram
##            prefix we are attempting to the predict the next word of. The
##            second column: freq, contains the frequency/count of each trigram.
## bigram - single row frequency table where the first col: ngram, is the bigram
##          which are the first two words of unobserved trigrams we want to
##          estimate probabilities of (same as bigPre in other functions listed
##          prior) delimited with '_'. The second column: freq, is the
##          frequency/count of the bigram listed in the ngram column.
## triDisc - amount to discount observed trigrams
getAlphaTrigram <- function(obsTrigs, bigram, triDisc=0.5) {
if(nrow(obsTrigs) < 1) return(1)
alphaTri <- 1 - sum((obsTrigs$freq - triDisc) / bigram$freq[1])
return(alphaTri)
}
## Returns a dataframe of 2 columns: ngram and prob.  Values in the ngram
## column are unobserved trigrams of the form: w3_w2_w1.  The values in the prob
## column are q_bo(w1 | w3, w2) calculated from equation 17.
##
## bigPre -  single-element char array of the form w2_w1 which are first two
##           words of the trigram we are predicting the tail word of
## qboObsBigrams - 2 column data.frame with the following columns -
##                 ngram: observed bigrams of the form w2_w1
##                 probs: the probability estimate for observed bigrams:
##                        qbo(w1 | w2) calc'd from equation 10.
## qboUnobsBigrams - 2 column data.frame with the following columns -
##                   ngram: unobserved bigrams of the form w2_w1
##                   probs: the probability estimate for unobserved bigrams
##                          qbo(w1 | w2) calc'd from equation 16.
## alphaTrig - total discounted probability mass at the trigram level
getUnobsTriProbs <- function(bigPre, qboObsBigrams,
qboUnobsBigrams, alphaTrig) {
qboBigrams <- rbind(qboObsBigrams, qboUnobsBigrams)
qboBigrams <- qboBigrams[order(-qboBigrams$prob), ]
sumQboBigs <- sum(qboBigrams$prob)
first_bigPre_word <- str_split(bigPre, "_")[[1]][1]
unobsTrigNgrams <- paste(first_bigPre_word, qboBigrams$ngram, sep="_")
unobsTrigProbs <- alphaTrig * qboBigrams$prob / sumQboBigs
unobsTrigDf <- data.frame(ngram=unobsTrigNgrams, prob=unobsTrigProbs)
return(unobsTrigDf)
}
getPredictionMsg <- function(qbo_trigs) {
# pull off tail word of highest prob trigram
prediction <- str_split(qbo_trigs$ngram[1], "_")[[1]][3]
result <- sprintf("%s%s%s%.4f", "highest prob prediction is >>> ", prediction," <<< which has probability = ", qbo_trigs$prob[1])
return(result)
}
predict_word <- function(bigPre,unigrs,bigrs,trigrs){
gamma2=0.7; gamma3=0.7  # initialize new discount rates
obs_trigs <- getObsTrigs(bigPre, trigrs)
unobs_trig_tails <- getUnobsTrigTails(obs_trigs$ngram, unigrs)
bo_bigrams <- getBOBigrams(bigPre, unobs_trig_tails)
# Separate bigrams into observed and unobserved using the appropriate equations
obs_bo_bigrams <- getObsBOBigrams(bigPre, unobs_trig_tails, bigrs)
unobs_bo_bigrams <- getUnobsBOBigrams(bigPre, unobs_trig_tails, obs_bo_bigrams)
# Calculate observed bigram probabilites
qbo_obs_bigrams <- getObsBigProbs(obs_bo_bigrams, unigrs, gamma2)
# Calculate alpha_big and unobserved bigram probabilities
unig <- str_split(bigPre, "_")[[1]][2]
unig <- unigrs[unigrs$ngram == unig,]
alpha_big <- getAlphaBigram(unig, bigrs, gamma2)
# Distribute discounted bigram probability mass to unobserved bigrams in   proportion to unigram ML
qbo_unobs_bigrams <- getQboUnobsBigrams(unobs_bo_bigrams, unigrs, alpha_big)
# Calculate observed trigram probabilities...
qbo_obs_trigrams <- getObsTriProbs(obs_trigs, bigrs, bigPre, gamma3)
# Finally, calculate unobserved trigram probabilities...
bigram <- bigrs[bigrs$ngram %in% bigPre, ]
alpha_trig <- getAlphaTrigram(obs_trigs, bigram, gamma3)
qbo_unobs_trigrams <- getUnobsTriProbs(bigPre, qbo_obs_bigrams,
qbo_unobs_bigrams, alpha_trig)
qbo_trigrams <- rbind(qbo_obs_trigrams, qbo_unobs_trigrams)
qbo_trigrams <- qbo_trigrams[order(-qbo_trigrams$prob), ]
# getPredictionMsg(qbo_trigrams)
return(qbo_trigrams)
}
bigPre = "bouquet_case"
word_list <- predict_word(bigPre,unigrs,bigrs,trigrs)
# word_list[grepl("bouquet_case_beer", ngram, fixed=TRUE)]
word_list[((word_list$ngram=="bouquet_case_beer") | (word_list$ngram=="bouquet_case_pretzels") | (word_list$ngram=="bouquet_case_cheese")| (word_list$ngram=="bouquet_case_soda")),]
bigPre = "would_mean"
word_list <- predict_word(bigPre,unigrs,bigrs,trigrs)
word_list[((word_list$ngram=="would_mean_world") | (word_list$ngram=="would_mean_best") | (word_list$ngram=="would_mean_most")| (word_list$ngram=="would_mean_universe")),]
bigPre = "make_me"
word_list <- predict_word(bigPre,unigrs,bigrs,trigrs)
word_list[((word_list$ngram=="make_me_bluest") | (word_list$ngram=="make_me_smelliest") | (word_list$ngram=="make_me_saddest")| (word_list$ngram=="make_me_happiest")),]
install.packages("rnn")
?rnn
??rnn
run.rnn_demo
library(rnn)
run.rnn_demo
install.packages("keras")
install.packages("tensorflow")
shiny::runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
runApp('Desktop/Data Science with R/Capstone Project/Week 5/Week 5 Assignment')
# Load the raw data files.
# These lines of code will take a little time to execute, so please be patient!
NEI <- readRDS("exdata-data-NEI_data/summarySCC_PM25.rds")
SCC <- readRDS("exdata-data-NEI_data/Source_Classification_Code.rds")
merged_df <- merge(NEI,SCC,by="SCC")
total_emissions <- aggregate(NEI$Emission, by=list(NEI$year), sum)
plot(total_emissions,pch=16,xlab="Year",ylab="Emissions (tons)",main="Total Emissions by Year")
lines(total_emissions$Group.1,total_emissions$x)
grid(lty="dotted")
echo = TRUE
# First, we run the following line of code to clean up the memory from any previous R sessions.
rm(list=ls(all=TRUE))
# Load the raw dataset
df <- read.csv("repdata-data-StormData.csv")
echo = TRUE
# Let's look at a few rows of the df...
head(df)
echo = TRUE
library(dplyr)
# Add numbers in 'FATALITIES' and 'INJURIES' columns
df$CASUALTIES <- df$FATALITIES+df$INJURIES
# Let's sort the df by 'CASUALTIES' descending
df <- df %>% arrange(-CASUALTIES)
echo = TRUE
# Aggregate the casualties by event type
casualties_per_event <- aggregate(df$CASUALTIES, by=list(df$EVTYPE), sum)
names(casualties_per_event) <- c("EVTYPE", "TOTAL_CASUALTIES")
casualties_per_event <- casualties_per_event %>% arrange(-TOTAL_CASUALTIES)
# Filter for events where the total casualties is greater than 1000
casualties_per_event <- casualties_per_event[casualties_per_event$TOTAL_CASUALTIES >= 1000,]
# Plot a column chart of Total Casualties vs. Event Type
library(ggplot2)
library(scales)
# png("total_casualties_by_event_type.png")
ggplot(data=casualties_per_event, aes(x = reorder(EVTYPE, -TOTAL_CASUALTIES), y = TOTAL_CASUALTIES)) + geom_bar(position="dodge",stat="identity",color="blue") + xlab("Event Type") + ylab("Number of Casualities") + theme(axis.text.x = element_text(angle = 270,size=15,vjust=0.5,color="blue"), axis.text.y = element_text(size=15,color="blue"),plot.title = element_text(size = 25, face = "bold",hjust=0.5),axis.title = element_text(size=15,face="bold")) + ggtitle("Total Casualties by Event Type") + geom_text(aes(label=TOTAL_CASUALTIES), position=position_dodge(width=0.5), size=5, vjust=-2)+scale_y_continuous(label=comma)
# dev.off()
echo = TRUE
# First, we run the following line of code to clean up the memory from any previous R sessions.
rm(list=ls(all=TRUE))
# Load the raw dataset
df <- read.csv("repdata-data-StormData.csv")
echo = TRUE
# Let's look at a few rows of the df...
head(df)
echo = TRUE
library(dplyr)
# Add numbers in 'FATALITIES' and 'INJURIES' columns
df$CASUALTIES <- df$FATALITIES+df$INJURIES
# Let's sort the df by 'CASUALTIES' descending
df <- df %>% arrange(-CASUALTIES)
echo = TRUE
# Aggregate the casualties by event type
casualties_per_event <- aggregate(df$CASUALTIES, by=list(df$EVTYPE), sum)
names(casualties_per_event) <- c("EVTYPE", "TOTAL_CASUALTIES")
casualties_per_event <- casualties_per_event %>% arrange(-TOTAL_CASUALTIES)
# Filter for events where the total casualties is greater than 1000
casualties_per_event <- casualties_per_event[casualties_per_event$TOTAL_CASUALTIES >= 1000,]
# Plot a column chart of Total Casualties vs. Event Type
library(ggplot2)
library(scales)
# png("total_casualties_by_event_type.png")
ggplot(data=casualties_per_event, aes(x = reorder(EVTYPE, -TOTAL_CASUALTIES), y = TOTAL_CASUALTIES)) + geom_bar(position="dodge",stat="identity",color="blue") + xlab("Event Type") + ylab("Number of Casualities") + theme(axis.text.x = element_text(angle = 270,size=15,vjust=0.5,color="blue"), axis.text.y = element_text(size=15,color="blue"),plot.title = element_text(size = 25, face = "bold",hjust=0.5),axis.title = element_text(size=15,face="bold")) + ggtitle("Total Casualties by Event Type") + geom_text(aes(label=TOTAL_CASUALTIES), position=position_dodge(width=0.5), size=5, vjust=-2)+scale_y_continuous(label=comma)
# dev.off()
plot(cars)
library(lubridate)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(chron)
# Read in the data file.
df <- read.csv("activity.csv")
# Convert 'date' column to datetime variable.
df$date <- as.Date(df$date,format="%Y-%m-%d")
# Convert 'interval' to a time variable
# df$interval <- times(sub("(.{2})", "\\1:", sprintf("%04d:00", df$interval)))
knitr::opts_chunk$set(echo = TRUE)
library(chron)
# Read in the data file.
df <- read.csv("activity.csv")
# Convert 'date' column to datetime variable.
df$date <- as.Date(df$date,format="%Y-%m-%d")
# Convert 'interval' to a time variable
# df$interval <- times(sub("(.{2})", "\\1:", sprintf("%04d:00", df$interval)))
# Drop rows with NA in 'steps' column
df1 <- df[complete.cases(df),]
# Calculate total steps per day
total_steps_per_day <- aggregate(df1$steps, by=list(df1$date), sum)
# Generate histogram
# png("Histogram of Total Steps Per Day.png")
hist(total_steps_per_day$x,breaks=20,col="blue",xlab="Total Steps Per Day",main="Histogram of Total Steps Per Day")
# dev.off()
# Calculate and report the mean and median of the total number of steps taken per day
mean <- mean(total_steps_per_day$x)
median <- median(total_steps_per_day$x)
paste0("Mean Total Steps per Day: ",round(mean,2))
paste0("Median Total Steps per Day: ",round(median,2))
library(ggplot2)
# Calculate mean number of steps for each time period
mean_steps <- aggregate(df1$steps, by=list(df1$interval), mean)
names(mean_steps) <- c("time","steps")
# Plot the line graph
# png("Mean Number of Steps by Time of Day.png")
ggplot(data=mean_steps, aes(x = time, y = steps))+geom_line()+xlab("Time of Day (HHMM)")+ggtitle("Mean Number of Steps by Time of Day")
# dev.off()
# Determine time interval when mean number of steps is maximum
mean_steps[which.max(mean_steps$steps), ]$time
# Calculate the number of rows with NAs
sum(!complete.cases(df))
library(plyr)
library(dplyr)
# Get rows in df where NAs are present
na_rows <- df[!complete.cases(df),]
# Rename columns
names(na_rows) <- c("steps","date","time")
# Impute mean number of steps for each time interval
na_rows <- merge(na_rows,mean_steps, by=c("time"))
# 'df1' is the original dataset with the NA rows deleted. We need to combine 'na_rows' with 'df1'. However, we need to clean up 'na_rows' a bit...
na_rows = subset(na_rows, select = -c(steps.x) )
names(na_rows) <- c("interval","date","steps")
# Combine 'df1' and 'na_rows'
df1 <- union(df1,na_rows)
# Sort 'df1' by date and then by interval
df1 <- df1 %>% arrange(date, interval)
# Calculate total steps per day
total_steps_per_day <- aggregate(df1$steps, by=list(df1$date), sum)
# Generate histogram
# png("Histogram of Total Steps Per Day with Imputed Values.png")
hist(total_steps_per_day$x,breaks=20,col="blue",xlab="Total Steps Per Day",main="Histogram of Total Steps Per Day")
# dev.off()
# Calculate and report the mean and median of the total number of steps taken per day
mean <- mean(total_steps_per_day$x)
median <- median(total_steps_per_day$x)
paste0("Mean Total Steps per Day: ",round(mean,2))
paste0("Median Total Steps per Day: ",round(median,2))
# Determine day of week for each date
df1$day <- weekdays(df1$date)
# Recode day of week to 'Weekday' or 'Weekend'
df1$weekday_or_weekend[df1$day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday")] <- "Weekday"
df1$weekday_or_weekend[df1$day %in% c("Saturday","Sunday")] <- "Weekend"
# Calculate mean number of steps for each time interval on Weekdays and Weekends
mean_steps <- aggregate(df1$steps, by=list(df1$weekday_or_weekend,df1$interval), mean)
names(mean_steps) <- c("Weekday_Weekend","time","steps")
# Sort 'mean_steps' by weekday/weekend and then by time
mean_steps <- mean_steps %>% arrange(Weekday_Weekend, time)
# The 'Weekday_Weekend' column must be converted to a factor variable for plotting
mean_steps$Weekday_Weekend <- as.factor(mean_steps$Weekday_Weekend)
# Create plots
# png("Mean Number of Steps by Time of Day Weekday Weekend.png")
ggplot(mean_steps, aes(x=time, y=steps))+geom_line()+facet_wrap(~Weekday_Weekend,nrow=2)+xlab("Time of Day")+ylab("Mean steps")+ggtitle("Mean Number of Steps by Time of Day")
# dev.off()
View(bigrs)
View(word_list)
demo()
knitr::opts_chunk$set(echo = TRUE)
# eCornell Hex Codes:
crimson = '#b31b1b' #Crimson
lightGray = '#cecece' #lightGray
darkGray = '#606366'
skyBlue = '#92b2c4' #skyblue
gold = '#fbb040' #gold
ecBlack = '#393f47' #ecBlack
# Load the data.
school = read.csv('mrc_table2.csv', header = TRUE, check.names = FALSE)
school = school[,names(school) %in%
c('name', 'type', 'tier', 'tier_name', 'mr_kq5_pq1',
'par_median', 'k_median')]
names(school)[5:7] <- c("mobility_rate", "parent_income", "student_income")
## Calculate the mean parent income for students at highly selective private schools:
school = school[school$tier_name %in% c("Highly selective private", "Highly selective public"),]
par.income.prv = school$parent_income[school$tier_name == 'Highly selective private']
mean(par.income.prv)
## Calculate the mean parent income for students at highly selective public schools.
par.income.pub = school$parent_income[school$tier_name == 'Highly selective public']
mean(par.income.pub)
# Create the boxplot:
boxplot(par.income.prv, par.income.pub, names = c('Highly selective private', 'Highly selective public'), ylab = 'Parent Income (USD)',
main = 'Parent Income: HS private and public schools', col = ecBlack)
# Create a permutation distribution:
set.seed(1)
P = 10000
store_mean_diff = rep(0, P)
for (n in 1:P){
school.perm = school
school.perm$parent_income = sample(school.perm$parent_income, replace = FALSE)
school.perm.prv = school.perm$parent_income[school.perm$tier_name == 'Highly selective private']
school.perm.pub = school.perm$parent_income[school.perm$tier_name == 'Highly selective public']
store_mean_diff[n] = mean(school.perm.prv) - mean(school.perm.pub)
}
# Plot the observed sample statistic on the histogram:
hist(store_mean_diff, breaks = 20, freq = FALSE, col = ecBlack, border = 'white',
xlab = 'Mean Diff of Parent Income',
main = 'Histogram of Parent Income Diff (Permuted Data)')
install.packages('tidyverse')
library(tidyverse)
library(tidyverse)
install.packages("tidyverse")
library("tidyverse")
install.packages("tidyverse")
help()
install.packages(c("nycflights13", "gapminder", "Lahman"))
myplot <- mpg
mpg
mpg
# install.packages("tidyverse")
library("tidyverse")
help()
install.packages(c("nycflights13", "gapminder", "Lahman"))
myplot <- load(mpg)
myplot <- load('mpg')
str(mpg)
myplot = mpg
View(myplot)
View(myplot)
myplot <- mpg
setwd("~/Desktop/Machine Learning and Data Analysis/eCornell_Data_Science_Essentials_With_R/CIS448 - Cleaning Data with TidyVerse/R Markdown Documents")
library(tidyverse)
library(utf8)
library(ellipsis)
Credit<- read.csv("credit_cards_v2.csv", stringsAsFactors = F)
head(Credit)
library(tidyverse)
library(utf8)
library(ellipsis)
Credit<- read.csv("credit_cards_v2.csv", stringsAsFactors = F)
head(Credit)
View(Credit)
Credit1 <- filter(Credit, Cards=1)
Credit1 <- filter(Credit, Cards==1)
head(Credit1)
Credit1$CreditUtil <- Credit1$Balance/Credit1$Limit
head(Credit1)
Credit1 %>% group_by(Married) %>% summarize(mean=mean(CreditUtil), sd=sd(CreditUtil))
install.packages("XQuartz")
install.packages("XQuartz")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
# read in the storm data:
storms <- read.csv("storms.csv")
# set the storm category to be a factor:
storms$Category <- factor(storms$Category, levels = -1:5)
# set the measurement date/time to be a factor:
storms$Date <- factor(storms$Date, levels = unique(storms$Date))
View(storms) # Look at the storms data
ggplot()
ggplot(storms, aes(x = TS_diameter, y = Pressure))
ggplot(storms, aes(x = TS_diameter, y = Pressure))+
geom_point()
ggplot(storms, aes(x = TS_diameter, y = Pressure))+
geom_point(size = 4)
theme <-  theme(plot.margin = margin(5, 5, 5, 5, "pt"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
panel.border = element_rect(colour = "#393f47", fill = NA, size = 2),
axis.text = element_text(size = 20),
axis.title.x = element_text(size = 24),
axis.title.y = element_text(size = 24),
plot.title = element_text(face = "bold", size = 30))
ourTheme <- list(theme, scale_color_manual(values = c('#393f47', '#b31b1b',
'#fbb040', '#92b2c4')))
ggplot(storms, aes(x = TS_diameter, y = Pressure))+
geom_point(size = 4) +
ourTheme
